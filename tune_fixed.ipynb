{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merinxavier/highcourt_project/blob/main/tune_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSWeYqofAEMv",
        "outputId": "e9f1d670-f167-4047-93e4-6913432503f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'malayalam-asr-datasets'...\n",
            "remote: Enumerating objects: 3321, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 3321 (delta 5), reused 21 (delta 5), pack-reused 3300 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3321/3321), 202.29 MiB | 16.67 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "Updating files: 100% (2199/2199), done.\n",
            "/content/malayalam-asr-datasets\n",
            "total 68\n",
            "drwxr-xr-x    4 root root  4096 Jun 24 11:37 .\n",
            "drwxr-xr-x    1 root root  4096 Jun 24 11:37 ..\n",
            "drwxr-xr-x    8 root root  4096 Jun 24 11:37 .git\n",
            "drwxr-xr-x 1101 root root 49152 Jun 24 11:37 NISP_MALAYALAM_CLEANED\n",
            "-rw-r--r--    1 root root  2828 Jun 24 11:37 README.md\n"
          ]
        }
      ],
      "source": [
        "# In Google Colab\n",
        "!git clone https://github.com/aswinpradeep/malayalam-asr-datasets.git\n",
        "%cd malayalam-asr-datasets\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVOYDppiAMl6",
        "outputId": "b8fc9ea8-84cc-4b22-86bb-f7ef2918888c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0051_Mal_f_0009/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0058_Mal_m_0011/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0037_Mal_m_0021/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0056_Mal_f_0026/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0055_Mal_f_0032/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0054_Mal_m_0026/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0047_Mal_m_0016/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0057_Mal_m_0024/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0041_Mal_f_0020/audio.txt\n",
            "/content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED/Mal_0053_Mal_m_0036/audio.txt\n"
          ]
        }
      ],
      "source": [
        "# Check if there are any CSV or metadata files\n",
        "!find /content/malayalam-asr-datasets/ -name \"*.csv\" -o -name \"*.json\" -o -name \"*.txt\" | head -10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86TT99bFASsr",
        "outputId": "8ad71e36-42e1-4438-a948-6702446e1c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Malayalam Speech Datasets\n",
            "\n",
            "The repository contains various cleaned Malayalam ASR ( Automated Speech Recognition ) corpus and points to several other openly available datasets which are directly usable for training Speech Recognition Models.\n",
            "\n",
            "All credits go to respective owners who have put efforts to create them. \n",
            "\n",
            "Please do contact for any ownership related issues or if you wish to contribute a new dataset.\n",
            "\n",
            "##  Labelled Datasets\n",
            "\n",
            "Datasets which has got audio chunks and their corresponding transcriptions.\n",
            "\n",
            "#### NISP DATASET\n",
            "\n",
            "* Crowd-sourced\n",
            "* ~25 speakers\n",
            "* ~2+ Hours\n",
            "* ~2200 utterances\n",
            "* Audio has Unnecessary pauses at the beginning\n",
            "* Repo contains cleaned version of Original dataset found [here](https://github.com/iiscleap/NISP-Dataset)\n",
            "* Can expect minor mismatch with very few audio files, even after cleanup.\n",
            "\n",
            "#### [Indic TTS Malayalam corpus](https://www.kaggle.com/kavyamanohar/indic-tts-malayalam-speech-corpus)\n",
            "\n",
            "* Studio recorded\n",
            "* 2 speakers\n",
            "* 8601 utterances\n",
            "* 13 hours 58 minutes 20 seconds\n",
            "* 48 kHz sampling rate\n",
            "\n",
            "#### [OpenSLR Malayalam](https://www.openslr.org/63/)\n",
            "\n",
            "* Studio Recorded HQ \n",
            "* ~40 speakers\n",
            "* ~5 Hours\n",
            "* 4100 utterances\n",
            "* 48 kHz sampling rate\n",
            "\n",
            "####  [Festvox IIITH Malayalam database](http://www.festvox.org/databases/iiit_voices/)\n",
            "\n",
            "* 1 speaker\n",
            "* 1000 utterances\n",
            "* 1 hour 38 minutes\n",
            "* 16 kHz sampling rate\n",
            "\n",
            "####  [MSC Reviewed speech](https://blog.smc.org.in/malayalam-speech-corpus/)\n",
            "\n",
            "* Recorded by volunteers in natural home/office/street environment with mobile devices:\n",
            "* 75 speakers\n",
            "* 1541 utterances\n",
            "* 1 hour 38 minutes 16 seconds\n",
            "* 48 kHz sampling rate\n",
            "\n",
            "####  [IIIT-H](http://cvit.iiit.ac.in/research/projects/cvit-projects/text-to-speech-dataset-for-indian-languages)\n",
            "\n",
            "* Available on request\n",
            "* Studio Recorded\n",
            "* 1 Speaker\n",
            "\n",
            "#### [NPLT-MALE](https://nplt.in/demo/mal-male-speech-corpus-iltts)\n",
            "\n",
            "* Paid dataset ( sample available for free )\n",
            "* ~18 hour+\n",
            "* English content included\n",
            "\n",
            "#### [NPLT-FEMALE](https://nplt.in/demo/mal-female-speech-corpus-iltts)\n",
            "\n",
            "* Paid dataset ( sample available for free )\n",
            "* ~17 hour+\n",
            "* English content included\n",
            "\n",
            "#### [Malayalam Raw Speech Corpus](https://nplt.in/demo/index.php?route=product/product&product_id=2005&search=malayalam+speech)\n",
            "\n",
            "* Paid Dataset ( sample available for free )\n",
            "* 164+ Hours\n",
            "* Complete metadata available\n",
            "* 458 speakers (231 Female and 227 Male)\n",
            "* 43670 utterances\n",
            "\n",
            "### [IIITM-K](https://aclanthology.org/2020.wildre-1.5/)\n",
            "\n",
            "* Openly unavailable & Unreachable\n",
            "* Sample is not available\n",
            "* Domain specific - Agriculture\n",
            "* Claims to have 250hr + data with proper meta info\n",
            "\n",
            "\n",
            "##  Un Labelled Datasets\n",
            "\n",
            "Basically, chunks of meaningful Malayalam audio which has got no text transcriptions\n",
            "\n",
            "\n",
            "#### [ULCA](https://github.com/Open-Speech-EkStep/ULCA-asr-dataset-corpus)\n",
            "\n",
            "*   600+ Hours\n",
            "*   Multiple Varieties of speakers\n",
            "*   Machine-generated\n"
          ]
        }
      ],
      "source": [
        "# Look at the README to understand the dataset structure\n",
        "!cat /content/malayalam-asr-datasets/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siwiQzKnAVzv",
        "outputId": "522e7928-e738-4945-d833-85d27b1f817a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No transcript file found. Scanning directory structure...\n",
            "Found 1099 audio files with transcriptions\n",
            "Loaded 1099 audio files\n",
            "Sample transcription: ‡¥µ‡¥ø‡¥±‡µç‡¥±‡¥æ‡¥Æ‡¥ø‡¥®‡µç‚Äç ‡¥é, ‡¥é, ‡¥¨‡¥ø, ‡¥ï‡¥æ‡¥§‡µç‡¥∏‡µç‡¥Ø‡¥Ç, ‡¥´‡µã‡¥∏‡µç‡¥´‡¥±‡¥∏‡µç‚Äå, ‡¥≤‡µÜ‡¥∏‡¥ø‡¥•‡¥ø‡¥®‡µç‚Äç, ‡¥á‡¥∞‡µÅ‡¥Æ‡µç‡¥™‡µç‚Äå ‡¥é‡¥®‡µç‡¥®‡¥ø‡¥µ‡¥Ø‡¥æ‡¥≤‡µç‚Äç ‡¥∏‡¥Æ‡µç‡¥®‡µç‡¥®‡¥µ‡µÉ‡¥Æ‡¥æ‡¥£‡µç‚Äå.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def load_malayalam_dataset(dataset_path):\n",
        "    \"\"\"Load Malayalam ASR dataset from the specified path\"\"\"\n",
        "    audio_files = []\n",
        "    transcriptions = []\n",
        "\n",
        "    # Look for common ASR dataset structures\n",
        "    audio_dir = os.path.join(dataset_path, \"audio\")  # or \"wav\", \"clips\"\n",
        "    transcript_file = os.path.join(dataset_path, \"transcripts.txt\")  # or \"transcript.csv\"\n",
        "\n",
        "    # Check if transcript file exists\n",
        "    if os.path.exists(transcript_file):\n",
        "        # Load transcriptions from file\n",
        "        with open(transcript_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            # Assuming format: filename|transcription\n",
        "            parts = line.strip().split('|')\n",
        "            if len(parts) >= 2:\n",
        "                filename = parts[0]\n",
        "                transcription = parts[1]\n",
        "\n",
        "                audio_path = os.path.join(audio_dir, filename)\n",
        "                if os.path.exists(audio_path):\n",
        "                    audio_files.append(audio_path)\n",
        "                    transcriptions.append(transcription)\n",
        "\n",
        "    # Alternative: Look for CSV file\n",
        "    elif os.path.exists(os.path.join(dataset_path, \"metadata.csv\")):\n",
        "        df = pd.read_csv(os.path.join(dataset_path, \"metadata.csv\"))\n",
        "        for _, row in df.iterrows():\n",
        "            audio_path = os.path.join(audio_dir, row['filename'])  # adjust column name\n",
        "            if os.path.exists(audio_path):\n",
        "                audio_files.append(audio_path)\n",
        "                transcriptions.append(row['transcription'])  # adjust column name\n",
        "\n",
        "    # Alternative: Scan directory structure\n",
        "    else:\n",
        "        print(\"No transcript file found. Scanning directory structure...\")\n",
        "        for audio_file in Path(dataset_path).rglob(\"*.wav\"):  # or *.mp3, *.flac\n",
        "            audio_files.append(str(audio_file))\n",
        "            # You'd need transcription files alongside audio files\n",
        "            transcript_file = audio_file.with_suffix('.txt')\n",
        "            if transcript_file.exists():\n",
        "                with open(transcript_file, 'r', encoding='utf-8') as f:\n",
        "                    transcriptions.append(f.read().strip())\n",
        "\n",
        "    print(f\"Found {len(audio_files)} audio files with transcriptions\")\n",
        "    return audio_files, transcriptions\n",
        "\n",
        "# Now load the dataset\n",
        "dataset_path = \"//content/malayalam-asr-datasets/NISP_MALAYALAM_CLEANED\"\n",
        "audio_files, transcriptions = load_malayalam_dataset(dataset_path)\n",
        "\n",
        "print(f\"Loaded {len(audio_files)} audio files\")\n",
        "if transcriptions:\n",
        "    print(f\"Sample transcription: {transcriptions[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9MlRNRpA6V8",
        "outputId": "b710f830-95cf-4ba7-e847-b50c3ff41448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required modules\n",
        "import re\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "def preprocess_malayalam_text(text: str) -> str:\n",
        "    \"\"\"Preprocess Malayalam text for ASR training\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Add more Malayalam-specific preprocessing as needed\n",
        "    # Remove punctuation except for essential ones\n",
        "    text = re.sub(r'[^\\u0d00-\\u0d7f\\s]', '', text)  # Keep only Malayalam characters and spaces\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_vocabulary(transcriptions: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"Create vocabulary from transcriptions\"\"\"\n",
        "    vocab = set()\n",
        "    for text in transcriptions:\n",
        "        if text:\n",
        "            vocab.update(text)\n",
        "\n",
        "    # Create vocab dict with special tokens\n",
        "    vocab_dict = {}\n",
        "    vocab_dict[\"<pad>\"] = 0\n",
        "    vocab_dict[\"<s>\"] = 1\n",
        "    vocab_dict[\"</s>\"] = 2\n",
        "    vocab_dict[\"<unk>\"] = 3\n",
        "    vocab_dict[\"|\"] = 4  # word separator\n",
        "\n",
        "    # Add all characters\n",
        "    for i, char in enumerate(sorted(vocab)):\n",
        "        vocab_dict[char] = i + 5\n",
        "\n",
        "    return vocab_dict\n",
        "\n",
        "print(\"Functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gma5hbQRBA6U",
        "outputId": "2c083bca-0b3f-4b13-fe93-33af0abab670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 69\n",
            "Sample vocabulary entries:\n",
            "'<pad>': 0\n",
            "'<s>': 1\n",
            "'</s>': 2\n",
            "'<unk>': 3\n",
            "'|': 4\n",
            "' ': 5\n",
            "'‡¥Ç': 6\n",
            "'‡¥É': 7\n",
            "'‡¥Ö': 8\n",
            "'‡¥Ü': 9\n",
            "'‡¥á': 10\n",
            "'‡¥à': 11\n",
            "'‡¥â': 12\n",
            "'‡¥é': 13\n",
            "'‡¥è': 14\n",
            "'‡¥ê': 15\n",
            "'‡¥í': 16\n",
            "'‡¥ì': 17\n",
            "'‡¥ï': 18\n",
            "'‡¥ñ': 19\n",
            "\n",
            "Sample original text: ‡¥µ‡¥ø‡¥±‡µç‡¥±‡¥æ‡¥Æ‡¥ø‡¥®‡µç‚Äç ‡¥é, ‡¥é, ‡¥¨‡¥ø, ‡¥ï‡¥æ‡¥§‡µç‡¥∏‡µç‡¥Ø‡¥Ç, ‡¥´‡µã‡¥∏‡µç‡¥´‡¥±‡¥∏‡µç‚Äå, ‡¥≤‡µÜ‡¥∏‡¥ø‡¥•‡¥ø‡¥®‡µç‚Äç, ‡¥á‡¥∞‡µÅ‡¥Æ‡µç‡¥™‡µç‚Äå ‡¥é‡¥®‡µç‡¥®‡¥ø‡¥µ‡¥Ø‡¥æ‡¥≤‡µç‚Äç ‡¥∏‡¥Æ‡µç‡¥®‡µç‡¥®‡¥µ‡µÉ‡¥Æ‡¥æ‡¥£‡µç‚Äå.\n",
            "Sample cleaned text: ‡¥µ‡¥ø‡¥±‡µç‡¥±‡¥æ‡¥Æ‡¥ø‡¥®‡µç ‡¥é ‡¥é ‡¥¨‡¥ø ‡¥ï‡¥æ‡¥§‡µç‡¥∏‡µç‡¥Ø‡¥Ç ‡¥´‡µã‡¥∏‡µç‡¥´‡¥±‡¥∏‡µç ‡¥≤‡µÜ‡¥∏‡¥ø‡¥•‡¥ø‡¥®‡µç ‡¥á‡¥∞‡µÅ‡¥Æ‡µç‡¥™‡µç ‡¥é‡¥®‡µç‡¥®‡¥ø‡¥µ‡¥Ø‡¥æ‡¥≤‡µç ‡¥∏‡¥Æ‡µç‡¥®‡µç‡¥®‡¥µ‡µÉ‡¥Æ‡¥æ‡¥£‡µç\n"
          ]
        }
      ],
      "source": [
        "# Continue with text preprocessing\n",
        "cleaned_transcriptions = [preprocess_malayalam_text(text) for text in transcriptions]\n",
        "\n",
        "# Create vocabulary\n",
        "vocab_dict = create_vocabulary(cleaned_transcriptions)\n",
        "print(f\"Vocabulary size: {len(vocab_dict)}\")\n",
        "\n",
        "# Show some vocabulary examples\n",
        "print(\"Sample vocabulary entries:\")\n",
        "for i, (char, idx) in enumerate(list(vocab_dict.items())[:20]):\n",
        "    print(f\"'{char}': {idx}\")\n",
        "\n",
        "print(f\"\\nSample original text: {transcriptions[0]}\")\n",
        "print(f\"Sample cleaned text: {cleaned_transcriptions[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDudEjvkBGvk",
        "outputId": "0b2b2c9c-308e-4583-fc84-605ee7713563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer and processor created successfully!\n",
            "Tokenizer vocab size: 69\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from transformers import (\n",
        "    Wav2Vec2CTCTokenizer,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForCTC,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import Dataset, Audio\n",
        "\n",
        "# Save vocabulary\n",
        "with open(\"/content/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\n",
        "    vocab_file=\"/content/vocab.json\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    word_delimiter_token=\"|\"\n",
        ")\n",
        "\n",
        "# Create feature extractor\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(\n",
        "    feature_size=1,\n",
        "    sampling_rate=16000,\n",
        "    padding_value=0.0,\n",
        "    do_normalize=True,\n",
        "    return_attention_mask=True\n",
        ")\n",
        "\n",
        "# Create processor\n",
        "processor = Wav2Vec2Processor(\n",
        "    feature_extractor=feature_extractor,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"Tokenizer and processor created successfully!\")\n",
        "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByP6JLVUBeEC",
        "outputId": "540948af-7db3-4aec-8e70-cb012c575137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All required functions imported!\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "def load_audio(audio_path: str, target_sr: int = 16000):\n",
        "    \"\"\"Load and resample audio file\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
        "        return audio\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {audio_path}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "print(\"All required functions imported!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dimgHw62BiUj",
        "outputId": "28339439-404c-4231-ba4c-c7237c0bec96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading audio files...\n",
            "Processing 0/1099 files...\n",
            "Processing 100/1099 files...\n",
            "Processing 200/1099 files...\n",
            "Processing 300/1099 files...\n",
            "Processing 400/1099 files...\n",
            "Processing 500/1099 files...\n",
            "Processing 600/1099 files...\n",
            "Processing 700/1099 files...\n",
            "Processing 800/1099 files...\n",
            "Processing 900/1099 files...\n",
            "Processing 1000/1099 files...\n",
            "Successfully loaded 1099 audio files\n",
            "Dataset size: 1099\n",
            "Train size: 879, Validation size: 220\n"
          ]
        }
      ],
      "source": [
        "# Dataset preparation\n",
        "def prepare_dataset(audio_files: List[str], transcriptions: List[str]):\n",
        "    \"\"\"Prepare dataset for training\"\"\"\n",
        "    dataset_dict = {\n",
        "        \"audio\": [],\n",
        "        \"text\": []\n",
        "    }\n",
        "\n",
        "    print(\"Loading audio files...\")\n",
        "    successful_loads = 0\n",
        "\n",
        "    for i, (audio_path, text) in enumerate(zip(audio_files, transcriptions)):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Processing {i}/{len(audio_files)} files...\")\n",
        "\n",
        "        if os.path.exists(audio_path) and text:\n",
        "            audio = load_audio(audio_path)\n",
        "            if len(audio) > 0:\n",
        "                dataset_dict[\"audio\"].append(audio)\n",
        "                dataset_dict[\"text\"].append(text)\n",
        "                successful_loads += 1\n",
        "\n",
        "    print(f\"Successfully loaded {successful_loads} audio files\")\n",
        "    return Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Create dataset\n",
        "dataset = prepare_dataset(audio_files, cleaned_transcriptions)\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "val_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i13_PcJBths",
        "outputId": "e0bb8266-1652-4e84-8c36-511e5e27f0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data preprocessing function ready!\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing function\n",
        "def prepare_dataset_for_training(batch):\n",
        "    \"\"\"Prepare batch for training\"\"\"\n",
        "    # Process audio\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_values\"] = processor(\n",
        "        audio,\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).input_values[0]\n",
        "\n",
        "    # Process text\n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "print(\"Data preprocessing function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "7138e2958d074a27a4d845848f0994e1",
            "2984f795811f4ea88f3517b3ba5c4313",
            "e216ece184f3455e9d336fe1109b576f",
            "47e730f85d314160bdf2a963d4aa4684",
            "51bfadf9d2f84bcc9a513384fec38816",
            "c2a85fff84ff42b3ad4f69d65cc86286",
            "11e8e18a0e1b48a0b24ffe0ab7db7744",
            "0e2d0adafffc4649bd8a7949138072c2",
            "4dddd440eefb41a090b04a85687f3686",
            "053ad845b71c4518883d6967324008bf",
            "fe9020e6b323439a824015d35e5d60ff",
            "f720ed5671d045af80320951531efe51",
            "3073ef1e61674e9d8adea0423570d2a9",
            "e9dbd430f3a94f4dbeba16d9e2e58e16",
            "d47f248628f142b093c6f534d37255a1",
            "1ea01d94fd3549f69f0eec40300327df",
            "0ea3cbf6f257424da065a94bc2fb9133",
            "aa2e33e5b53e4bd4b8ee454ea4a4fa4d",
            "4faca36985304720a045b35d6232825d",
            "a1157c28f8384cc89d3aa65f4c71bb6c",
            "18650f0dffde4f7390281fa26dfddd2b",
            "bbc5d78a0bc24a68bad1a485589c42f0"
          ]
        },
        "id": "NCKaP009BybU",
        "outputId": "523d4567-14b1-4a71-e222-628a34c86eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing training dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7138e2958d074a27a4d845848f0994e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/879 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing validation dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f720ed5671d045af80320951531efe51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset preprocessing completed!\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing to datasets\n",
        "print(\"Preprocessing training dataset...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    prepare_dataset_for_training,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Preprocessing validation dataset...\")\n",
        "val_dataset = val_dataset.map(\n",
        "    prepare_dataset_for_training,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Dataset preprocessing completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNUtXWqVCReY",
        "outputId": "14e250a0-7b16-409c-9dcb-f7f7276ea503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data collator created!\n"
          ]
        }
      ],
      "source": [
        "# Data collator\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: bool = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        # Split inputs and labels\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        labels_batch = self.processor.pad(\n",
        "            labels=label_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "# Create data collator\n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
        "\n",
        "print(\"Data collator created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "b91bd5854d21485791e099d0d3ccdc68",
            "492a0495c81841528724ee009233456f",
            "19dc03be02b6412680fb3a8293551264",
            "1394125399bb4efdb96c96fe6bdea2df",
            "a135df1ca0894e7bae4c88e26d52b795",
            "fcd4b1c18ddc4ac7a31f26b4cbb26293",
            "3093997197a2455698c71c1259002611",
            "cb9186f9f41f4c31b04eff1a60d9ce1e",
            "99864fafa41d4f2fb3a7998aafadb055",
            "a55e91cf5e5b49c59238ce155e440b50",
            "52af5f0ebd9a47f6b5676f3fdaeb92b5",
            "d85dbd0908e84a72a393edd739fc394d",
            "5db2743cc911485ba1af27e2e75ea345",
            "66ee4ee6ac5a46709b05b15fee8fb745",
            "1e64ea4db78940a38f25b9e2fb10d0aa",
            "08a2c46488ba4734aea2046cddf8a3b3",
            "cb87eb4d84c24ef7880604e70d6d1cd2",
            "67dd379e973240239e33aad4055e2a9b",
            "1ec90784516047efac728a223a3b4270",
            "7c83341b50354738bc5d8837200e3ba4",
            "e7692b5c05e14fe6af1f4a0d8d2edf4f",
            "fcb3d39b76254f479af09f8ce1e05062",
            "b88e71d994bd4ad4b1d81c7358498046",
            "6f183ccba17f4332a3c309d951ce1532",
            "293892f0c7574682bb649841fc979929",
            "b186276590d6475186cebd01948b7097",
            "8673fb947404431c8a9fe505948ffe0d",
            "d96b8a8687c14935b3ac581d30681958",
            "280dc101c3a642babf49d1c4a5cc87ff",
            "c18ad0e94cab45d5aaf59a867487a0c4",
            "091fbf26e0a2414d8c072bcbcd9be69e",
            "9d9e84e9fee74e54aff7d48c7fe196d5",
            "530b720577b1413a8b3fa7f2531fda80"
          ]
        },
        "id": "srhH_aOxCXyV",
        "outputId": "7e6cd10e-ac1d-4ba6-8b13-de31f7d315b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading pre-trained Wav2Vec2 model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b91bd5854d21485791e099d0d3ccdc68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.77k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d85dbd0908e84a72a393edd739fc394d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b88e71d994bd4ad4b1d81c7358498046",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded and ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load pre-trained Wav2Vec2 model\n",
        "print(\"Loading pre-trained Wav2Vec2 model...\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\",\n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")\n",
        "\n",
        "# Freeze feature extractor\n",
        "model.freeze_feature_encoder()\n",
        "\n",
        "# Move to device\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded and ready for training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXRV8ggpCuap",
        "outputId": "fb8c7e12-4417-4ecc-df1c-93b2a7581e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataCollator defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# All necessary imports\n",
        "import torch\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Union\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import pandas as pd\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Union[int, None] = None\n",
        "    max_length_labels: Union[int, None] = None\n",
        "    pad_to_multiple_of: Union[int, None] = None\n",
        "    pad_to_multiple_of_labels: Union[int, None] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Split inputs and labels since they have to be of different lengths and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # Replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "print(\"DataCollator defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "46oKhethEVBs",
        "outputId": "94c05311-ec1a-4ef1-b460-d48b24626537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--kP3HzXEp0g",
        "outputId": "181c83b4-5be8-4663-af34-7de76ce6b9f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Evaluation metrics defined!\n"
          ]
        }
      ],
      "source": [
        "import jiwer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"Compute Word Error Rate (WER) for evaluation\"\"\"\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Replace -100 with pad token id\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Compute WER\n",
        "    try:\n",
        "        wer = jiwer.wer(label_str, pred_str)\n",
        "    except:\n",
        "        # Fallback to character-level accuracy if WER calculation fails\n",
        "        total_chars = sum(len(label) for label in label_str)\n",
        "        if total_chars == 0:\n",
        "            wer = 0.0\n",
        "        else:\n",
        "            correct_chars = sum(\n",
        "                sum(c1 == c2 for c1, c2 in zip(pred, label))\n",
        "                for pred, label in zip(pred_str, label_str)\n",
        "            )\n",
        "            wer = 1.0 - (correct_chars / total_chars)\n",
        "\n",
        "    print(f\"üìä Current WER: {wer:.4f}\")\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "print(\"‚úÖ Evaluation metrics defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x62dwW54FCrv",
        "outputId": "bb600146-8b96-4e14-bd61-486eea7d5a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "KPFmrHHVJS5i",
        "outputId": "dea592da-91cc-4fbe-cda3-b7ad63068b74"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [330/330 11:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "If still getting OOM, try these additional steps:\n",
            "1. Further reduce batch size to 1\n",
            "2. Increase gradient_accumulation_steps\n",
            "3. Use CPU offloading (see code below)\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Clear GPU cache before training\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Define TrainingArguments with memory-optimized settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=1,  # Reduced to 1 to save memory\n",
        "    per_device_eval_batch_size=2,   # Small eval batch size\n",
        "    gradient_accumulation_steps=8,  # Simulate larger batch size (effective batch = 1*8=8)\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=100,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # Memory optimization settings\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,  # Trade compute for memory\n",
        "    fp16=True,  # Use mixed precision to save memory\n",
        "    remove_unused_columns=False,\n",
        "\n",
        "    # Additional memory saving options\n",
        "    dataloader_num_workers=0,  # Reduce CPU memory usage\n",
        "    load_best_model_at_end=False,  # Don't keep best model in memory\n",
        ")\n",
        "\n",
        "# Create trainer with the correct parameter name\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    processing_class=processor,  # Use processing_class instead of tokenizer\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Additional memory optimization before training\n",
        "if hasattr(model, 'gradient_checkpointing_enable'):\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "# Start training\n",
        "try:\n",
        "    trainer.train()\n",
        "except torch.cuda.OutOfMemoryError:\n",
        "    print(\"Still out of memory. Try the alternative approach below.\")\n",
        "\n",
        "# Alternative: Even more aggressive memory saving\n",
        "print(\"\\nIf still getting OOM, try these additional steps:\")\n",
        "print(\"1. Further reduce batch size to 1\")\n",
        "print(\"2. Increase gradient_accumulation_steps\")\n",
        "print(\"3. Use CPU offloading (see code below)\")\n",
        "\n",
        "# Ultra-low memory training configuration\n",
        "training_args_ultra_low = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,  # Higher accumulation\n",
        "    num_train_epochs=3,\n",
        "    save_steps=1000,\n",
        "    eval_strategy=\"epoch\",  # Evaluate less frequently\n",
        "    logging_steps=50,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=50,\n",
        "    save_total_limit=1,  # Keep only 1 checkpoint\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # Maximum memory optimization\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_num_workers=0,\n",
        "    load_best_model_at_end=False,\n",
        "    eval_accumulation_steps=1,  # Process eval in smaller chunks\n",
        ")\n",
        "\n",
        "# Uncomment to use ultra-low memory settings:\n",
        "# trainer_ultra_low = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args_ultra_low,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     processing_class=processor,\n",
        "#     data_collator=data_collator,\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "# trainer_ultra_low.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j11Y9k1KHcgX",
        "outputId": "cd767daa-f20a-4892-8195-d253834ce18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving trained model...\n",
            "‚úÖ Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Save the final trained model\n",
        "print(\"Saving trained model...\")\n",
        "trainer.save_model(\"./wav2vec2-finetuned-final\")\n",
        "processor.save_pretrained(\"./wav2vec2-finetuned-final\")\n",
        "print(\"‚úÖ Model saved successfully!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndgLiurnJIYq",
        "outputId": "1995a3ae-0730-4d33-dbf9-90b8352a7159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving trained model to Google Drive...\n",
            "‚úÖ Model saved successfully to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "# Define the path to save in Google Drive\n",
        "drive_save_path = \"/content/drive/MyDrive/wav2vec2-finetuned-final\"\n",
        "\n",
        "# Save the final trained model to Google Drive\n",
        "print(\"Saving trained model to Google Drive...\")\n",
        "trainer.save_model(drive_save_path)\n",
        "processor.save_pretrained(drive_save_path)\n",
        "print(\"‚úÖ Model saved successfully to Google Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxUvaSWbitsF",
        "outputId": "64893895-7820-462b-b167-65447b4165e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 0s (1,823 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y portaudio19-dev\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1TikKCAiEpn",
        "outputId": "9e387974-6206-43d6-d1a1-0a577b7f0e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sounddevice\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
            "Downloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice\n",
            "Successfully installed sounddevice-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install sounddevice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyoi_SMFi6Na",
        "outputId": "f9b60b80-841e-455e-d4bb-4f3880704abb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sounddevice\n",
            "  Using cached sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting CFFI>=1.0 (from sounddevice)\n",
            "  Using cached cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting pycparser (from CFFI>=1.0->sounddevice)\n",
            "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Using cached sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Using cached cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (467 kB)\n",
            "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "Installing collected packages: pycparser, CFFI, sounddevice\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.22\n",
            "    Uninstalling pycparser-2.22:\n",
            "      Successfully uninstalled pycparser-2.22\n",
            "  Attempting uninstall: CFFI\n",
            "    Found existing installation: cffi 1.17.1\n",
            "    Uninstalling cffi-1.17.1:\n",
            "      Successfully uninstalled cffi-1.17.1\n",
            "  Attempting uninstall: sounddevice\n",
            "    Found existing installation: sounddevice 0.5.2\n",
            "    Uninstalling sounddevice-0.5.2:\n",
            "      Successfully uninstalled sounddevice-0.5.2\n",
            "Successfully installed CFFI-1.17.1 pycparser-2.22 sounddevice-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install sounddevice --force-reinstall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXnYTMOlJ7mX",
        "outputId": "ff4a66ef-58c2-40fa-d685-5b5ab936eb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MALAYALAM REAL-TIME TRANSCRIPTION SYSTEM\n",
            "============================================================\n",
            "\n",
            "STEP-BY-STEP INSTRUCTIONS:\n",
            "1. Run: test_setup()      # Test if packages work\n",
            "2. Run: test_model()      # Test if model loads\n",
            "3. Run: main()            # Start transcription\n",
            "\n",
            "Make sure your model is saved at:\n",
            "/content/drive/MyDrive/wav2vec2-finetuned-final\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sounddevice as sd\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import librosa\n",
        "from collections import deque\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class RealTimeMalayalamTranscriber:\n",
        "    def __init__(self, model_path, sample_rate=16000, chunk_duration=2.0, overlap=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the real-time transcriber\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to your trained model (e.g., \"/content/drive/MyDrive/wav2vec2-finetuned-final\")\n",
        "            sample_rate: Audio sample rate (16kHz is standard for Wav2Vec2)\n",
        "            chunk_duration: Duration of each audio chunk in seconds\n",
        "            overlap: Overlap between chunks in seconds\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.sample_rate = sample_rate\n",
        "        self.chunk_duration = chunk_duration\n",
        "        self.overlap = overlap\n",
        "        self.chunk_size = int(sample_rate * chunk_duration)\n",
        "        self.overlap_size = int(sample_rate * overlap)\n",
        "\n",
        "        # Audio buffer and processing queue\n",
        "        self.audio_buffer = deque(maxlen=int(sample_rate * 10))  # 10 seconds buffer\n",
        "        self.audio_queue = queue.Queue()\n",
        "        self.transcription_queue = queue.Queue()\n",
        "\n",
        "        # Control flags\n",
        "        self.is_recording = False\n",
        "        self.is_processing = False\n",
        "\n",
        "        # Load model and processor\n",
        "        print(\"Loading trained model...\")\n",
        "        self.load_model()\n",
        "        print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the trained Wav2Vec2 model and processor\"\"\"\n",
        "        try:\n",
        "            self.processor = Wav2Vec2Processor.from_pretrained(self.model_path)\n",
        "            self.model = Wav2Vec2ForCTC.from_pretrained(self.model_path)\n",
        "            self.model.eval()\n",
        "\n",
        "            # Move to GPU if available\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            self.model.to(self.device)\n",
        "            print(f\"Model loaded on: {self.device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def audio_callback(self, indata, frames, time, status):\n",
        "        \"\"\"Callback function for audio input\"\"\"\n",
        "        if status:\n",
        "            print(f\"Audio input status: {status}\")\n",
        "\n",
        "        # Convert to mono and add to buffer\n",
        "        audio_data = indata[:, 0] if indata.ndim > 1 else indata\n",
        "        self.audio_buffer.extend(audio_data)\n",
        "\n",
        "        # Add to processing queue if we have enough data\n",
        "        if len(self.audio_buffer) >= self.chunk_size:\n",
        "            chunk = np.array(list(self.audio_buffer)[-self.chunk_size:])\n",
        "            if not self.audio_queue.full():\n",
        "                self.audio_queue.put(chunk)\n",
        "\n",
        "    def transcribe_chunk(self, audio_chunk):\n",
        "        \"\"\"Transcribe a single audio chunk\"\"\"\n",
        "        try:\n",
        "            # Preprocess audio\n",
        "            if len(audio_chunk) == 0:\n",
        "                return \"\"\n",
        "\n",
        "            # Normalize audio\n",
        "            audio_chunk = audio_chunk / np.max(np.abs(audio_chunk)) if np.max(np.abs(audio_chunk)) > 0 else audio_chunk\n",
        "\n",
        "            # Process with the model\n",
        "            with torch.no_grad():\n",
        "                inputs = self.processor(\n",
        "                    audio_chunk,\n",
        "                    sampling_rate=self.sample_rate,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True\n",
        "                )\n",
        "\n",
        "                inputs = inputs.to(self.device)\n",
        "                logits = self.model(inputs.input_values).logits\n",
        "\n",
        "                # Decode the transcription\n",
        "                predicted_ids = torch.argmax(logits, dim=-1)\n",
        "                transcription = self.processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "                return transcription.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Transcription error: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def processing_worker(self):\n",
        "        \"\"\"Worker thread for processing audio chunks\"\"\"\n",
        "        while self.is_processing:\n",
        "            try:\n",
        "                # Get audio chunk with timeout\n",
        "                audio_chunk = self.audio_queue.get(timeout=1.0)\n",
        "\n",
        "                # Transcribe the chunk\n",
        "                transcription = self.transcribe_chunk(audio_chunk)\n",
        "\n",
        "                if transcription and transcription.strip():\n",
        "                    timestamp = time.strftime(\"%H:%M:%S\")\n",
        "                    self.transcription_queue.put((timestamp, transcription))\n",
        "\n",
        "                self.audio_queue.task_done()\n",
        "\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Processing error: {e}\")\n",
        "\n",
        "    def display_worker(self):\n",
        "        \"\"\"Worker thread for displaying transcriptions\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üé§ REAL-TIME MALAYALAM TRANSCRIPTION\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"Press Ctrl+C to stop\\n\")\n",
        "\n",
        "        while self.is_processing:\n",
        "            try:\n",
        "                timestamp, transcription = self.transcription_queue.get(timeout=1.0)\n",
        "                print(f\"[{timestamp}] {transcription}\")\n",
        "                self.transcription_queue.task_done()\n",
        "\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Display error: {e}\")\n",
        "\n",
        "    def start_transcription(self):\n",
        "        \"\"\"Start real-time transcription\"\"\"\n",
        "        try:\n",
        "            print(\"Starting real-time transcription...\")\n",
        "            print(\"Make sure your microphone is working!\")\n",
        "\n",
        "            # Set flags\n",
        "            self.is_recording = True\n",
        "            self.is_processing = True\n",
        "\n",
        "            # Start worker threads\n",
        "            processing_thread = threading.Thread(target=self.processing_worker, daemon=True)\n",
        "            display_thread = threading.Thread(target=self.display_worker, daemon=True)\n",
        "\n",
        "            processing_thread.start()\n",
        "            display_thread.start()\n",
        "\n",
        "            # Start audio stream\n",
        "            with sd.InputStream(\n",
        "                samplerate=self.sample_rate,\n",
        "                channels=1,\n",
        "                callback=self.audio_callback,\n",
        "                blocksize=int(self.sample_rate * 0.1),  # 100ms blocks\n",
        "                dtype=np.float32\n",
        "            ):\n",
        "                print(\"üé§ Listening... Speak in Malayalam!\")\n",
        "\n",
        "                # Keep running until interrupted\n",
        "                while self.is_recording:\n",
        "                    time.sleep(0.1)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n‚èπÔ∏è  Stopping transcription...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during transcription: {e}\")\n",
        "        finally:\n",
        "            self.stop_transcription()\n",
        "\n",
        "    def stop_transcription(self):\n",
        "        \"\"\"Stop real-time transcription\"\"\"\n",
        "        self.is_recording = False\n",
        "        self.is_processing = False\n",
        "        print(\"‚úÖ Transcription stopped!\")\n",
        "\n",
        "# Test setup function\n",
        "def test_setup():\n",
        "    \"\"\"Test if everything is set up correctly\"\"\"\n",
        "    try:\n",
        "        import sounddevice as sd\n",
        "        import torch\n",
        "        from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "        print(\"‚úÖ All required packages imported successfully!\")\n",
        "        print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "        print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "        # Test audio devices\n",
        "        print(\"\\nüé§ Available audio devices:\")\n",
        "        devices = sd.query_devices()\n",
        "        for i, device in enumerate(devices):\n",
        "            if device['max_input_channels'] > 0:\n",
        "                print(f\"  Input Device {i}: {device['name']}\")\n",
        "\n",
        "        return True\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå Missing package: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Setup error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Main function to run transcription\n",
        "def main():\n",
        "    \"\"\"Main function to run the transcription system\"\"\"\n",
        "    # Configuration\n",
        "    MODEL_PATH = \"/content/drive/MyDrive/wav2vec2-finetuned-final\"  # Update this path\n",
        "\n",
        "    try:\n",
        "        # Check if model exists\n",
        "        if not os.path.exists(MODEL_PATH):\n",
        "            print(f\"‚ùå Model not found at: {MODEL_PATH}\")\n",
        "            print(\"Please check your model path!\")\n",
        "            return\n",
        "\n",
        "        print(\"Initializing transcriber...\")\n",
        "        transcriber = RealTimeMalayalamTranscriber(\n",
        "            model_path=MODEL_PATH,\n",
        "            sample_rate=16000,\n",
        "            chunk_duration=3.0,  # Process 3-second chunks\n",
        "            overlap=0.5          # 0.5 second overlap\n",
        "        )\n",
        "\n",
        "        # Start transcription\n",
        "        transcriber.start_transcription()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        print(\"Make sure your model path is correct and the model files exist!\")\n",
        "\n",
        "# Quick model test function\n",
        "def test_model(model_path=None):\n",
        "    \"\"\"Test if the model can be loaded\"\"\"\n",
        "    if model_path is None:\n",
        "        model_path = \"/content/drive/MyDrive/wav2vec2-finetuned-final\"\n",
        "\n",
        "    try:\n",
        "        print(f\"Testing model at: {model_path}\")\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"‚ùå Model directory not found!\")\n",
        "            return False\n",
        "\n",
        "        # List files in model directory\n",
        "        print(\"Files in model directory:\")\n",
        "        for file in os.listdir(model_path):\n",
        "            print(f\"  - {file}\")\n",
        "\n",
        "        # Try to load model\n",
        "        processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
        "        model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
        "\n",
        "        print(\"‚úÖ Model loaded successfully!\")\n",
        "        print(f\"‚úÖ Vocabulary size: {len(processor.tokenizer)}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "        return False\n",
        "\n",
        "# Instructions\n",
        "print(\"=\"*60)\n",
        "print(\"MALAYALAM REAL-TIME TRANSCRIPTION SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"STEP-BY-STEP INSTRUCTIONS:\")\n",
        "print(\"1. Run: test_setup()      # Test if packages work\")\n",
        "print(\"2. Run: test_model()      # Test if model loads\")\n",
        "print(\"3. Run: main()            # Start transcription\")\n",
        "print()\n",
        "print(\"Make sure your model is saved at:\")\n",
        "print(\"/content/drive/MyDrive/wav2vec2-finetuned-final\")\n",
        "print(\"=\"*60\n",
        "      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbVSzayFK3mb"
      },
      "source": [
        "test_setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_5J-1eJK_hJ",
        "outputId": "b7111898-4ade-4ece-da4c-1e9c7198082a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing model at: /content/drive/MyDrive/wav2vec2-finetuned-final\n",
            "Files in model directory:\n",
            "  - preprocessor_config.json\n",
            "  - vocab.json\n",
            "  - special_tokens_map.json\n",
            "  - model.safetensors\n",
            "  - config.json\n",
            "  - training_args.bin\n",
            "  - tokenizer_config.json\n",
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Vocabulary size: 69\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL4eDrxLlNrA",
        "outputId": "08c58633-1b3b-4110-b495-a98ed31eff67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.14.0)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.3\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70CV798BSJ5N",
        "outputId": "e16f7449-de8f-4838-e1c8-a8172adace89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "import io\n",
        "from transformers import WhisperForConditionalGeneration, WhisperTokenizer, WhisperProcessor\n",
        "from datasets import Audio\n",
        "import speech_recognition as sr\n",
        "from IPython.display import Audio as IPAudio, display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1LFOeioSSeP",
        "outputId": "7ac45e86-59d6-4d4d-96ac-99bc23723c22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"Using CPU - transcription will be slower\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwZNnBcqSZK5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MalayalamTranscriber:\n",
        "    def __init__(self, model_path_or_name=\"openai/whisper-small\"):\n",
        "        \"\"\"\n",
        "        Initialize the Malayalam transcriber\n",
        "\n",
        "        Options for model_path_or_name:\n",
        "        1. \"openai/whisper-small\" - General multilingual model\n",
        "        2. \"openai/whisper-base\" - Faster but less accurate\n",
        "        3. \"openai/whisper-medium\" - Better accuracy\n",
        "        4. \"/path/to/your/trained/model\" - Your custom trained model\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.model_name = model_path_or_name\n",
        "\n",
        "        print(f\"Loading model: {model_path_or_name}\")\n",
        "\n",
        "        try:\n",
        "            # Load tokenizer and processor\n",
        "            self.tokenizer = WhisperTokenizer.from_pretrained(model_path_or_name)\n",
        "            self.processor = WhisperProcessor.from_pretrained(model_path_or_name)\n",
        "\n",
        "            # Load model\n",
        "            self.model = WhisperForConditionalGeneration.from_pretrained(\n",
        "                model_path_or_name,\n",
        "                torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n",
        "            ).to(device)\n",
        "\n",
        "            print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            print(\"Falling back to Whisper base model...\")\n",
        "            self.tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\")\n",
        "            self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "            self.model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(device)\n",
        "\n",
        "    def transcribe_audio(self, audio_path_or_array, language=\"ml\"):\n",
        "        \"\"\"\n",
        "        Transcribe audio to Malayalam text\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load audio\n",
        "            if isinstance(audio_path_or_array, str):\n",
        "                audio, sr = librosa.load(audio_path_or_array, sr=16000)\n",
        "            else:\n",
        "                audio = audio_path_or_array\n",
        "                if len(audio.shape) > 1:\n",
        "                    audio = audio.mean(axis=1)  # Convert to mono if stereo\n",
        "\n",
        "            # Process audio\n",
        "            inputs = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            # Generate transcription\n",
        "            with torch.no_grad():\n",
        "                generated_ids = self.model.generate(\n",
        "                    inputs.input_features,\n",
        "                    language=language,\n",
        "                    task=\"transcribe\",\n",
        "                    max_length=448,\n",
        "                    num_beams=5,\n",
        "                    temperature=0.0\n",
        "                )\n",
        "\n",
        "            # Decode transcription\n",
        "            transcription = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "            return transcription\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during transcription: {e}\")\n",
        "            return \"Error in transcription\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06a38d1b",
        "outputId": "f6b9767d-41b5-45b7-ff57-f69c561ca2d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['wav2vec2-finetuned-final']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "# Check current directory\n",
        "os.listdir('.')\n",
        "# Check if mounted to Google Drive\n",
        "os.listdir('/content/drive/MyDrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "29521cac8cb447ceb1d5c3e9918566a8",
            "b54791387ea24621888d374c1abc2940",
            "90dc2818787744b6a4c00508ec487603",
            "c4ba40f057774313b19603fb4e99c450",
            "ca11da870eb94432898750434da93ac4",
            "327d6ce91c134cafac98afee7fe74617",
            "1e37dd45d80f411180a1dbe574bd89e3",
            "027d04608178499ead6f62c8d5865bab",
            "d231405a07944f289795efeae95a4822",
            "8bfd689d8ad849bd898242ef83ecd73d",
            "787de69ff77043fdb2df6127f853b99a",
            "32c415dd2e55446dbd66406f5360f53a",
            "4b7b99173bd14c7f9bf1fb34f832cab3",
            "ec9b0124ef3d4b6db50026cfd1e7844f",
            "d95f52d2e1004abe998eb3b17a5ef22c",
            "04c1af93b3b844afbde9431ff27d282b",
            "5093284e3cb74999a3cb9d3a459d6917",
            "c66b338fc72f41d580f183ac665359cb",
            "ea5e2ff2d1e94932abb7b5410d78dac9",
            "a167fef3ed2a4aad8e4935617cd9871b",
            "e011c424bea44442bddb2dd30d7c68bd",
            "90294311cea7481792902b7f3ced144e",
            "fb7d24c5cd9c4e46a02b84038cc1ecd1",
            "d6b05aa7a5db460e8b9c960ce8371e31",
            "108df8fe25ea49408c3f519271ca555e",
            "e88a76e17e0141b4b290216fef629239",
            "f713fa6086124765a45b22c5234b98ac",
            "263816889ac24352842a24cea79c5b9b",
            "155054afa86f42ebac52979b2f2c0f0d",
            "c780ea2944ec433c88f5360659094fd9",
            "29fb91e957f145e889a5f53c5f805ecf",
            "222a54e5b8c3469a88b65bd8ea71e957",
            "e931368bf9434ebeb689bd16d803c5e4",
            "a4042c581c1b4927bbd794b55b99a80b",
            "29114846995b4aaaaf1079fe27ab1355",
            "b699787089244c2ea5ccc624dfdbe0e7",
            "182b7990e8324a9ba278e3fdbb3c5425",
            "5d4e7548d03e411398d4ed422c6ca0ce",
            "5d981210e46e4d92a63a01d79e346be2",
            "c4a34057d12b42c581ccb2294629bc7c",
            "e224da5cf63b42169ddddd3c9ca77427",
            "7cd6c35ce6b74be587625a414a1ec846",
            "f852a313d8ac41a79052d8359dcd16a7",
            "22990310849b44d08ee97b7495f94db6",
            "2e0624e91cd64308b630819534b9e134",
            "0fde7dc22dd14879b23262d71750c2b1",
            "1634e98e427b477095794cb3b7c4279d",
            "5023a93a40614bf6908811e5464bb81e",
            "c84e260787e14760a3bf7ba26c75ba0f",
            "e0e4568391fa4d3f95cc845c7b86bb21",
            "35620224dbb843f8ae29c379f87fa183",
            "8097f5f6cbd84c0aa32949a79d41838c",
            "500c5944dda746568e507af11f9bda87",
            "00a5182e9b4f4d5eb96545024c359484",
            "20fedaecf68541a19474de1d6cde04b2",
            "e8c28ecc0a3b4c0e955d70726b491304",
            "0f628f8384b04a02bb8aece9bb898b17",
            "856a81f6753943f199d779f78cf7b162",
            "39a5e446ad604e3a91acd429f21bcc52",
            "214b62de769a4aaa83f43c3fd66f74d5",
            "a9213388ff48448a824d8c0a30a71d85",
            "e02aa3d261684a46bac92678a7b61727",
            "84c17db10c87481d80080142546c87c6",
            "03cec4485fff4ba5a7210d222d95b04b",
            "383b7fdd3b8f45f2991425d064caced6",
            "96a181eab5024c71bdb8f272ba9aa738",
            "0e1312f0a9be4893847b6c2e4ff5cf62",
            "79aef0a8b72640959bc4f28527ecebc0",
            "f71f3b39199948569378559b8f9f2ed0",
            "88995a5f0a4f4a558ff46abdbdc506ea",
            "cfe06b5570fb4fc18ceeb76bcf95b7ee",
            "fae7537e245748b595710b2b399ee8d5",
            "6592137dccb14f5f940cbb13dd5ba702",
            "a0be5022a3f64aabb707d4cec3362eb9",
            "dd55029352aa494f8d6a670bcdac1870",
            "bac689bf318644b388b66699697aef28",
            "dfdbd85f2e2242498ce186a8dbbf5917"
          ]
        },
        "id": "RV4IEOEf3_YD",
        "outputId": "79907ac5-512d-4621-d177-710b8780363a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29521cac8cb447ceb1d5c3e9918566a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32c415dd2e55446dbd66406f5360f53a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb7d24c5cd9c4e46a02b84038cc1ecd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4042c581c1b4927bbd794b55b99a80b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e0624e91cd64308b630819534b9e134",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8c28ecc0a3b4c0e955d70726b491304",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e1312f0a9be4893847b6c2e4ff5cf62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "# Hugging Face pretrained base model (you can choose multilingual or Indian fine-tuned models too)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "# Optional: Save to your Drive to avoid redownloading\n",
        "model.save_pretrained(\"/content/drive/MyDrive/wav2vec2-base\")\n",
        "processor.save_pretrained(\"/content/drive/MyDrive/wav2vec2-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoqiLhe1Aedn",
        "outputId": "f5f3d34e-b29e-4cec-da5c-80992964c05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Unmounted and cleared /content/drive\n",
            "Cleared Google auth cache\n",
            "Attempting to mount Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mount command executed.\n",
            "‚úÖ Google Drive successfully mounted!\n"
          ]
        }
      ],
      "source": [
        "# First, try to unmount if already mounted\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.flush_and_unmount()\n",
        "    # Explicitly remove contents after unmount attempt\n",
        "    import os\n",
        "    import shutil\n",
        "    mount_point = '/content/drive'\n",
        "    if os.path.exists(mount_point):\n",
        "        for item in os.listdir(mount_point):\n",
        "            item_path = os.path.join(mount_point, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "    print(\"Unmounted and cleared /content/drive\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during unmount/clear: {e}\")\n",
        "    pass # Ignore errors if it wasn't mounted or already empty\n",
        "\n",
        "# Clear any cached credentials\n",
        "import os\n",
        "os.system('rm -rf ~/.config/Google')\n",
        "print(\"Cleared Google auth cache\")\n",
        "\n",
        "# Now try mounting again\n",
        "from google.colab import drive\n",
        "print(\"Attempting to mount Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Google Drive mount command executed.\")\n",
        "\n",
        "# You can add a check here to see if it was successful\n",
        "import time\n",
        "time.sleep(2) # Give it a moment to complete mounting process\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"‚úÖ Google Drive successfully mounted!\")\n",
        "else:\n",
        "    print(\"‚ùå Google Drive might not have mounted correctly. Check the output above for errors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv37v7MxBGmW",
        "outputId": "86477217-9cf8-4cc2-d6df-20dc0ea18b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of Google Drive:\n",
            "['1000110161-1.jpg', 'ID Card (1).pdf', 'Classroom', 'Untitled folder', 'Merin Xavier .jpg', 'Merin Xavier.png', '1688753156 (1).pdf', '1688753156.pdf', 'Colab Notebooks', 'eFKfV.pdf', 'InShot_20230827_143536846.jpg', 'certificate.png', 'ID Card.pdf', '1709014577.pdf', '1713527106.pdf', 'IMG20240419172404.jpg', 'Screenshot_2024-04-19-17-38-26-43_e2d5b3f32b79de1d45acd1fad96fbb0f.jpg', 'DOC-20240511-WA0031..gdoc', '1_INTRODUCTION TO OS.gslides', '2_System Calls.gslides', '3-Operating System Structures.gslides', 'Relational Algebra.gdoc', 'MODULE 4 -Normalization_1.gslides', 'ID card (2).pdf', 'Screenshot_2024-09-11-21-05-24-98_e307a3f9df9f380ebaf106e1dc980bb6.jpg', 'Full_Stack_Development_using_MERN-AT_AI_IN_FSD_AMALJYOTHI_01_JULY_2024_BATCH_02_GROUP_CERTIFICATE__R_22_SEP_17819.pdf', 'Consent letter CHE (1).gdoc', 'Consent letter CT.gdoc', 'Consent letter CHE.gdoc', 'NASA certificate.pdf', '35NpqCeX6K (1).pdf', 'Merin Xavier- Introduction to Cybersecurity (1).pdf', 'Merin Xavier- Introduction to Cybersecurity.pdf', 'IPL.gsheet', 'exp2.pkt', 'FirstReview.gslides', 'IMG_20250426_203305.jpg', 'IMG_20250426_203329.jpg', 'IMG_20250426_203424.jpg']\n",
            "Model folder not found. Let's search for it...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check what's in your Google Drive\n",
        "print(\"Contents of Google Drive:\")\n",
        "drive_contents = os.listdir(\"/content/drive/MyDrive/\")\n",
        "print(drive_contents)\n",
        "\n",
        "# Look for your wav2vec2 model folder\n",
        "model_path = \"/content/drive/MyDrive/wav2vec2-finetuned-final\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"\\nModel folder found at: {model_path}\")\n",
        "    print(\"Model files:\")\n",
        "    model_files = os.listdir(model_path)\n",
        "    for file in model_files:\n",
        "        print(f\"  - {file}\")\n",
        "else:\n",
        "    print(\"Model folder not found. Let's search for it...\")\n",
        "    for item in drive_contents:\n",
        "        if \"wav2vec2\" in item.lower():\n",
        "            print(f\"Found wav2vec2-related folder: {item}\")\n",
        "            model_path = f\"/content/drive/MyDrive/{item}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL5S1qACG_dh",
        "outputId": "6c38d2a7-27d2-4be9-8f81-0fb15ae25da1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Path does not exist.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/wav2vec2-finetuned-final\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"üìÇ Folder exists!\")\n",
        "    print(\"üìÑ Contents:\")\n",
        "    print(os.listdir(model_path))\n",
        "else:\n",
        "    print(\"‚ùå Path does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDdmAYoYGfZu",
        "outputId": "029f86be-3545-47ae-a8b6-c2e4cb64e376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "Model loading failed: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/wav2vec2-finetuned-final'. Use `repo_type` argument if needed.\n",
            "Double check that the path to your model folder is correct.\n",
            "Ensure that the 'config.json' file and other necessary model files are directly inside this folder:\n",
            "/content/drive/MyDrive/wav2vec2-finetuned-final\n",
            "\n",
            "The specified model directory does not exist.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/wav2vec2-finetuned-final\"  # your local path\n",
        "\n",
        "# Load your fine-tuned model\n",
        "print(\"Loading model...\")\n",
        "# The error suggests the path format is the issue.\n",
        "# Ensure the path is correct and retry.\n",
        "# The local_files_only=True flag is crucial for local loading.\n",
        "try:\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(model_path, local_files_only=True)\n",
        "    print(\"‚úì Model loaded successfully!\")\n",
        "\n",
        "    # Load the processor\n",
        "    try:\n",
        "        processor = Wav2Vec2Processor.from_pretrained(model_path, local_files_only=True)\n",
        "        print(\"‚úì Processor loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Processor loading failed: {e}\")\n",
        "        print(\"You may need to load the processor from the base model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Model loading failed: {e}\")\n",
        "    print(\"Double check that the path to your model folder is correct.\")\n",
        "    print(\"Ensure that the 'config.json' file and other necessary model files are directly inside this folder:\")\n",
        "    print(model_path)\n",
        "    # You can add a check here to list files in the directory to confirm\n",
        "    import os\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"\\nFiles in the specified model directory:\")\n",
        "        print(os.listdir(model_path))\n",
        "    else:\n",
        "        print(\"\\nThe specified model directory does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SdkN3CSBa-L",
        "outputId": "68607fa5-6f1b-4ebf-d3b7-f039f28d5644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model type: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC'>\n",
            "Model config: Wav2Vec2Config {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_attn_dim\": null,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForCTC\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 768,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"mean\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.0,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"mask_channel_length\": 10,\n",
            "  \"mask_channel_min_space\": 1,\n",
            "  \"mask_channel_other\": 0.0,\n",
            "  \"mask_channel_prob\": 0.0,\n",
            "  \"mask_channel_selection\": \"static\",\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_min_space\": 1,\n",
            "  \"mask_time_other\": 0.0,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"mask_time_selection\": \"static\",\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"proj_codevector_dim\": 768,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 69,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n",
            "Model device: cpu\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model type: {type(model)}\")\n",
        "print(f\"Model config: {model.config}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q08ES-N8yeiB",
        "outputId": "30211abf-6ea6-404c-b7ba-01c378191ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1HzgCYxBpDW",
        "outputId": "091f9f94-ad9e-4863-c5fb-32f5a1cb36ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Model is now on: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(device)\n",
        "print(f\"Model is now on: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "873bf096834a4128ad6cdbf71adcd50a",
            "51f970ed1324466ab0dbe00bc5a4352b",
            "090e6e534769423fb98bb9b3d2128d69",
            "0b3eda06752d4828b345efbd7a9d2744",
            "42ab6bc878994bb8afc9f8506475a9a0",
            "67fa25fe8b3e4e8da34fbe46a2b142de",
            "ea0d48e252884b5888a47787b89a25ed",
            "c61ce72e74be4a0a8208d1c623e90204",
            "3e380c24f696434e9702adbf8b68831a",
            "e786129c7d4743908936288d6d24f40d",
            "10b6a2bf403747e4847b6dba93dcbc95",
            "0877ff03889f4abd8fc4102ded097cdb",
            "7d5c69397cb04e2985f4649ec395291b",
            "c5cd9c86e8004f1db1c98a1a83926d2a",
            "ca101de974ab44caa6acd020211c9508",
            "710fea23466848f4abb800b8ff592ee4",
            "0cf46cf7f60b4b8cb76cbf7c6e988849",
            "3397efdb24db4c74a8026593cd7f7974",
            "6b3bfd7351bf40d396938a9e13bc5046",
            "7bb3abd26e0541ba9371b78f9a0a10a3",
            "1580bfed11dc4b9fb6530a7d5bf90370",
            "c218244418c2421bb5d68ab2c82fa27e",
            "1aadcf6d63e34c05816feb94ea6b876a",
            "582ea915534646699e41ae9561f150ac",
            "6cd150d831904b9790fb2212884c390d",
            "19b0a4a2d54a4961aa9c49b8ff8b3a28",
            "5e7cac06e088430a8bf19c9e7472eb56",
            "02a681bea42549c38ffa92b4282e4289",
            "12916a39e95e4f348e481b10da74df2c",
            "92c31cf87a224a6a92ccae044cf90e15",
            "29071c7eb7764eafb763f4b05fe3dceb",
            "f9372278dacc4dee90f5f4d3e65bed73",
            "0f7110cfc177459db6b6f448008cf231",
            "e20970173487402b96b1a4eb495975f2",
            "93391a88210f4f3f9a883935e28c7d44",
            "5343b07045e14a73a40d4441b9d129fe",
            "22196a73d5c9468e84a29439758fd3c7",
            "7c93e106c952494f8125c12983cf5d68",
            "8f24e5dcd8484be290adacbd633acd88",
            "31001cccc3174c7887d3311cf409ef0a",
            "6ab16b35b87c4da3bcc15c866e25c28c",
            "11a9f48b6c22479fa11f0e4212d7bbf1",
            "9cd40628f0614ee9946d73715e0e10f8",
            "95b0b7b1016c462f87ca2b39052d8f57",
            "f8f3b192a4b245ec960fcf4155d607e9",
            "2f9d6bbaa3f643cebdef6dc3fa6830b2",
            "4333c0c1f2804089af22c93b6c377685",
            "131cab1696c54cc08ecaf4ec5a102ddd",
            "fcb571de0aae4a5fa26f6a7410aa9024",
            "db989a93a4924c6eab405adc17a10dc1",
            "468dbfd0723f4808bf03c5722c7a4fcc",
            "25fcb5622bd54f5e82d98de04b15545a",
            "0d23601cfdcc4c8c8661ab95731affac",
            "2b6e2a10a56f4fab889b9175ad0b3a8d",
            "dd3307db3a4c4123a4f43424bd5d0536"
          ]
        },
        "id": "ZCba-eHTBten",
        "outputId": "5f6da28e-7303-4c5a-9a22-dbb406f2fcfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:61: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processor not found in saved model, loading from base model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "873bf096834a4128ad6cdbf71adcd50a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0877ff03889f4abd8fc4102ded097cdb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aadcf6d63e34c05816feb94ea6b876a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e20970173487402b96b1a4eb495975f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8f3b192a4b245ec960fcf4155d607e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Processor loaded from base model\n"
          ]
        }
      ],
      "source": [
        "# Try to load the processor that was saved with your model\n",
        "try:\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
        "    print(\"‚úì Processor loaded from your saved model\")\n",
        "except:\n",
        "    print(\"Processor not found in saved model, loading from base model...\")\n",
        "    from transformers import Wav2Vec2Processor\n",
        "    # You might need to specify the base model you used for training\n",
        "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    print(\"‚úì Processor loaded from base model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl8USCvHBxAV",
        "outputId": "2cef80f6-0398-4ba6-9892-92bf707fe803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is ready for inference!\n",
            "You can now use transcribe_audio('path_to_your_audio_file.wav') to transcribe audio\n"
          ]
        }
      ],
      "source": [
        "# Function to transcribe audio\n",
        "def transcribe_audio(audio_path):\n",
        "    import librosa\n",
        "\n",
        "    # Load audio\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Process audio\n",
        "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Move inputs to same device as model\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # Decode predictions\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "    return transcription\n",
        "\n",
        "print(\"Model is ready for inference!\")\n",
        "print(\"You can now use transcribe_audio('path_to_your_audio_file.wav') to transcribe audio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3OED9d4CJ5S"
      },
      "source": [
        "# Real-time Malayalam Audio Transcription using your fine-tuned Wav2Vec2 model\n",
        "\n",
        "# Step 1: Install required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNQQlzrTCMJy",
        "outputId": "3048114a-2920-47f9-925e-6223f97e32dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "portaudio19-dev is already the newest version (19.6.0-1.1).\n",
            "Suggested packages:\n",
            "  python-pyaudio-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pyaudio\n",
            "0 upgraded, 1 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 25.9 kB of archives.\n",
            "After this operation, 117 kB of additional disk space will be used.\n",
            "Selecting previously unselected package python3-pyaudio.\n",
            "(Reading database ... 126352 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-pyaudio_0.2.11-1.3ubuntu1_amd64.deb ...\n",
            "Unpacking python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Setting up python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Requirement already satisfied: pyaudio in /usr/lib/python3/dist-packages (0.2.11)\n",
            "Collecting ipywebrtc\n",
            "  Downloading ipywebrtc-0.6.0-py2.py3-none-any.whl.metadata (825 bytes)\n",
            "Downloading ipywebrtc-0.6.0-py2.py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ipywebrtc\n",
            "Successfully installed ipywebrtc-0.6.0\n",
            "Collecting javascript\n",
            "  Downloading javascript-1%211.2.2-py3-none-any.whl.metadata (14 kB)\n",
            "Downloading javascript-1%211.2.2-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: javascript\n",
            "Successfully installed javascript-1!1.2.2\n"
          ]
        }
      ],
      "source": [
        "!apt update -qq\n",
        "!apt install portaudio19-dev python3-pyaudio -qq\n",
        "!pip install pyaudio\n",
        "!pip install ipywebrtc\n",
        "!pip install javascript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF-dJ32e5epI",
        "outputId": "c5e5dac3-ef9d-4456-a79f-e6e3de350047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "The token `whisper _ai _token` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `whisper _ai _token`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eldNJE1y6-Dh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W3UMiUvFDDyt",
        "outputId": "4299fb61-40b3-4895-a098-fde609ebf524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "üîÑ Loading Malayalam speech recognition model...\n",
            "‚úÖ Model loaded successfully on cuda\n",
            "üì± Model: gvs/wav2vec2-large-xlsr-malayalam\n",
            "üöÄ Complete Malayalam Speech Recognition System\n",
            "============================================================\n",
            "üî¥ Real-time: Start recording and speak in Malayalam\n",
            "üìÅ File Upload: Select and upload audio files for transcription\n",
            "üîÑ Long files will be processed in chunks automatically\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"max-width: 800px; margin: 0 auto; font-family: Arial, sans-serif;\">\n",
              "    <h2 style=\"text-align: center; color: #2E7D32;\">üé§ Malayalam Audio Transcription System</h2>\n",
              "\n",
              "    <!-- Real-time Recording Section -->\n",
              "    <div style=\"margin: 20px 0; padding: 20px; border: 2px solid #4CAF50; border-radius: 10px; background-color: #f9f9f9;\">\n",
              "        <h3 style=\"color: #2E7D32; margin-top: 0;\">üî¥ Real-time Recording</h3>\n",
              "        <button id=\"startBtn\" onclick=\"startRealTimeRecording()\" style=\"background-color: #4CAF50; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; margin: 5px;\">\n",
              "            Start Real-time Recording\n",
              "        </button>\n",
              "        <button id=\"stopBtn\" onclick=\"stopRealTimeRecording()\" disabled style=\"background-color: #f44336; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; margin: 5px;\">\n",
              "            Stop Recording\n",
              "        </button>\n",
              "        <div id=\"realtime-status\" style=\"font-size: 16px; font-weight: bold; margin: 10px 0;\">Ready to record Malayalam audio...</div>\n",
              "        <div id=\"realtime-transcription\" style=\"margin-top: 10px; padding: 10px; background-color: white; border: 1px solid #ddd; border-radius: 5px; min-height: 80px; max-height: 200px; overflow-y: auto;\"></div>\n",
              "    </div>\n",
              "\n",
              "    <!-- File Upload Section -->\n",
              "    <div style=\"margin: 20px 0; padding: 20px; border: 2px solid #2196F3; border-radius: 10px; background-color: #f3f8ff;\">\n",
              "        <h3 style=\"color: #1976D2; margin-top: 0;\">üìÅ Audio File Upload</h3>\n",
              "        <div style=\"margin: 15px 0;\">\n",
              "            <input type=\"file\" id=\"audioFileInput\" accept=\"audio/*\" style=\"margin: 10px 0; padding: 5px;\">\n",
              "            <button id=\"uploadBtn\" onclick=\"uploadAndTranscribe()\" style=\"background-color: #2196F3; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px;\">\n",
              "                Upload & Transcribe\n",
              "            </button>\n",
              "        </div>\n",
              "        <div style=\"font-size: 14px; color: #666;\">\n",
              "            Supported formats: MP3, WAV, M4A, OGG, FLAC, WebM (Max size: 100MB)\n",
              "        </div>\n",
              "        <div id=\"upload-status\" style=\"font-size: 16px; font-weight: bold; margin: 10px 0;\">Select an audio file to transcribe...</div>\n",
              "        <div id=\"upload-transcription\" style=\"margin-top: 10px; padding: 10px; background-color: white; border: 1px solid #ddd; border-radius: 5px; min-height: 80px; max-height: 300px; overflow-y: auto; white-space: pre-wrap;\"></div>\n",
              "    </div>\n",
              "\n",
              "    <!-- Instructions -->\n",
              "    <div style=\"margin: 20px 0; padding: 15px; background-color: #fff3e0; border-radius: 5px; border-left: 4px solid #ff9800;\">\n",
              "        <h4 style=\"color: #ef6c00; margin-top: 0;\">üìã Instructions:</h4>\n",
              "        <ul style=\"color: #bf360c;\">\n",
              "            <li><strong>Real-time:</strong> Click \"Start Recording\" and speak in Malayalam</li>\n",
              "            <li><strong>File Upload:</strong> Select an audio file and click \"Upload & Transcribe\"</li>\n",
              "            <li>Long files (>30 seconds) will be processed in chunks</li>\n",
              "            <li>Ensure clear audio quality for better transcription</li>\n",
              "        </ul>\n",
              "    </div>\n",
              "</div>\n",
              "\n",
              "<script>\n",
              "// Real-time recording variables\n",
              "let mediaRecorder;\n",
              "let isRecording = false;\n",
              "let audioChunks = [];\n",
              "let recordingInterval;\n",
              "\n",
              "// Real-time recording functions\n",
              "async function startRealTimeRecording() {\n",
              "    try {\n",
              "        const stream = await navigator.mediaDevices.getUserMedia({\n",
              "            audio: {\n",
              "                sampleRate: 16000,\n",
              "                channelCount: 1,\n",
              "                echoCancellation: true,\n",
              "                noiseSuppression: true\n",
              "            }\n",
              "        });\n",
              "\n",
              "        mediaRecorder = new MediaRecorder(stream, {\n",
              "            mimeType: 'audio/webm;codecs=opus'\n",
              "        });\n",
              "\n",
              "        isRecording = true;\n",
              "        document.getElementById('startBtn').disabled = true;\n",
              "        document.getElementById('stopBtn').disabled = false;\n",
              "        document.getElementById('realtime-status').innerHTML = 'üî¥ Recording... Speak in Malayalam!';\n",
              "        document.getElementById('realtime-transcription').innerHTML = '';\n",
              "\n",
              "        // Process audio in chunks every 3 seconds\n",
              "        recordingInterval = setInterval(() => {\n",
              "            if (isRecording && mediaRecorder.state === 'recording') {\n",
              "                mediaRecorder.stop();\n",
              "                setTimeout(() => {\n",
              "                    if (isRecording) {\n",
              "                        mediaRecorder.start();\n",
              "                    }\n",
              "                }, 100);\n",
              "            }\n",
              "        }, 3000);\n",
              "\n",
              "        mediaRecorder.ondataavailable = function(event) {\n",
              "            if (event.data.size > 0) {\n",
              "                audioChunks.push(event.data);\n",
              "                processRealtimeAudioChunk(event.data);\n",
              "            }\n",
              "        };\n",
              "\n",
              "        mediaRecorder.start();\n",
              "\n",
              "    } catch (err) {\n",
              "        console.error('Error accessing microphone:', err);\n",
              "        document.getElementById('realtime-status').innerHTML = '‚ùå Error: Cannot access microphone';\n",
              "    }\n",
              "}\n",
              "\n",
              "function stopRealTimeRecording() {\n",
              "    isRecording = false;\n",
              "\n",
              "    if (mediaRecorder && mediaRecorder.state !== 'inactive') {\n",
              "        mediaRecorder.stop();\n",
              "        mediaRecorder.stream.getTracks().forEach(track => track.stop());\n",
              "    }\n",
              "\n",
              "    if (recordingInterval) {\n",
              "        clearInterval(recordingInterval);\n",
              "    }\n",
              "\n",
              "    document.getElementById('startBtn').disabled = false;\n",
              "    document.getElementById('stopBtn').disabled = true;\n",
              "    document.getElementById('realtime-status').innerHTML = '‚èπÔ∏è Recording stopped';\n",
              "}\n",
              "\n",
              "async function processRealtimeAudioChunk(audioBlob) {\n",
              "    try {\n",
              "        const reader = new FileReader();\n",
              "        reader.onload = function() {\n",
              "            const base64Audio = reader.result.split(',')[1];\n",
              "            google.colab.kernel.invokeFunction('transcribe_audio', [base64Audio], {})\n",
              "                .then(result => {\n",
              "                    if (result.data && result.data.trim()) {\n",
              "                        const transcriptionDiv = document.getElementById('realtime-transcription');\n",
              "                        transcriptionDiv.innerHTML += result.data + '<br>';\n",
              "                        transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;\n",
              "                    }\n",
              "                });\n",
              "        };\n",
              "        reader.readAsDataURL(audioBlob);\n",
              "    } catch (error) {\n",
              "        console.error('Error processing real-time audio chunk:', error);\n",
              "    }\n",
              "}\n",
              "\n",
              "// File upload functions\n",
              "async function uploadAndTranscribe() {\n",
              "    const fileInput = document.getElementById('audioFileInput');\n",
              "    const file = fileInput.files[0];\n",
              "\n",
              "    if (!file) {\n",
              "        document.getElementById('upload-status').innerHTML = '‚ö†Ô∏è Please select an audio file first';\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    // Validate file size (100MB max)\n",
              "    if (file.size > 100 * 1024 * 1024) {\n",
              "        document.getElementById('upload-status').innerHTML = '‚ùå File too large. Maximum size is 100MB';\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    // Validate file type\n",
              "    const allowedTypes = ['audio/mpeg', 'audio/wav', 'audio/mp4', 'audio/ogg', 'audio/webm', 'audio/flac', 'audio/m4a'];\n",
              "    if (!allowedTypes.some(type => file.type.includes('audio'))) {\n",
              "        document.getElementById('upload-status').innerHTML = '‚ùå Please select a valid audio file';\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    document.getElementById('upload-status').innerHTML = 'üîÑ Processing audio file... Please wait';\n",
              "    document.getElementById('upload-transcription').innerHTML = 'Processing...';\n",
              "    document.getElementById('uploadBtn').disabled = true;\n",
              "\n",
              "    try {\n",
              "        const reader = new FileReader();\n",
              "        reader.onload = function() {\n",
              "            const base64Audio = reader.result.split(',')[1];\n",
              "            google.colab.kernel.invokeFunction('transcribe_file', [base64Audio, file.name], {})\n",
              "                .then(result => {\n",
              "                    document.getElementById('upload-status').innerHTML = '‚úÖ Transcription completed';\n",
              "                    document.getElementById('upload-transcription').innerHTML = result.data || 'No transcription result';\n",
              "                    document.getElementById('uploadBtn').disabled = false;\n",
              "                })\n",
              "                .catch(error => {\n",
              "                    console.error('Transcription error:', error);\n",
              "                    document.getElementById('upload-status').innerHTML = '‚ùå Error during transcription';\n",
              "                    document.getElementById('upload-transcription').innerHTML = 'Transcription failed. Please try again.';\n",
              "                    document.getElementById('uploadBtn').disabled = false;\n",
              "                });\n",
              "        };\n",
              "        reader.readAsDataURL(file);\n",
              "    } catch (error) {\n",
              "        console.error('File reading error:', error);\n",
              "        document.getElementById('upload-status').innerHTML = '‚ùå Error reading file';\n",
              "        document.getElementById('uploadBtn').disabled = false;\n",
              "    }\n",
              "}\n",
              "\n",
              "// File input change handler\n",
              "document.addEventListener('DOMContentLoaded', function() {\n",
              "    const fileInput = document.getElementById('audioFileInput');\n",
              "    if (fileInput) {\n",
              "        fileInput.addEventListener('change', function() {\n",
              "            const file = this.files[0];\n",
              "            if (file) {\n",
              "                const fileSize = (file.size / (1024 * 1024)).toFixed(2);\n",
              "                document.getElementById('upload-status').innerHTML =\n",
              "                    `üìÅ Selected: ${file.name} (${fileSize} MB) - Ready to transcribe`;\n",
              "            }\n",
              "        });\n",
              "    }\n",
              "});\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Complete Malayalam Audio Transcription System\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "from scipy.io import wavfile\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import output\n",
        "import os\n",
        "\n",
        "# Install required packages\n",
        "!pip install pydub transformers datasets\n",
        "\n",
        "import pydub\n",
        "from pydub import AudioSegment\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Initialize model and processor for Malayalam\n",
        "print(\"üîÑ Loading Malayalam speech recognition model...\")\n",
        "\n",
        "# You can replace this with your specific Malayalam model\n",
        "# Popular options:\n",
        "MODEL_NAME = \"gvs/wav2vec2-large-xlsr-malayalam\"  # Example Malayalam model\n",
        "# MODEL_NAME = \"your-custom-malayalam-model\"  # Replace with your model\n",
        "\n",
        "try:\n",
        "    # Load processor and model\n",
        "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"‚úÖ Model loaded successfully on {device}\")\n",
        "    print(f\"üì± Model: {MODEL_NAME}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"Please make sure you have the correct model name or path\")\n",
        "    print(\"\\nüîß Alternative: Load your custom model\")\n",
        "    print(\"If you have a custom model, replace the MODEL_NAME with:\")\n",
        "    print(\"- Your model path (e.g., './my-malayalam-model')\")\n",
        "    print(\"- Your Hugging Face model name\")\n",
        "    print(\"- Or use a different Malayalam model from Hugging Face\")\n",
        "\n",
        "    # Alternative model loading (uncomment and modify as needed)\n",
        "    \"\"\"\n",
        "    # Example for custom model loading:\n",
        "    from transformers import AutoProcessor, AutoModelForCTC\n",
        "    processor = AutoProcessor.from_pretrained(\"your-model-path\")\n",
        "    model = AutoModelForCTC.from_pretrained(\"your-model-path\")\n",
        "    \"\"\"\n",
        "\n",
        "    raise\n",
        "\n",
        "# Real-time transcription function\n",
        "def transcribe_realtime_audio(base64_audio):\n",
        "    \"\"\"Process and transcribe real-time audio\"\"\"\n",
        "    try:\n",
        "        # Decode base64 audio\n",
        "        audio_data = base64.b64decode(base64_audio)\n",
        "\n",
        "        # Use pydub to handle WebM format\n",
        "        audio_io = io.BytesIO(audio_data)\n",
        "\n",
        "        # Load audio with pydub (handles WebM)\n",
        "        audio_segment = AudioSegment.from_file(audio_io, format=\"webm\")\n",
        "\n",
        "        # Convert to mono and set sample rate to 16kHz\n",
        "        audio_segment = audio_segment.set_channels(1).set_frame_rate(16000)\n",
        "\n",
        "        # Convert to numpy array\n",
        "        audio_np = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)\n",
        "\n",
        "        # Normalize\n",
        "        audio_float = audio_np / 32768.0\n",
        "\n",
        "        # Skip if audio is too short\n",
        "        if len(audio_float) < 8000:  # Less than 0.5 seconds\n",
        "            return \"\"\n",
        "\n",
        "        # Transcribe with your model\n",
        "        inputs = processor(audio_float, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcription = processor.batch_decode(predicted_ids)[0].strip()\n",
        "\n",
        "        # Only return if transcription is meaningful\n",
        "        if transcription and len(transcription) > 1:\n",
        "            print(f\"üó£Ô∏è Malayalam: {transcription}\")\n",
        "            return transcription\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in real-time transcription: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# File upload transcription function\n",
        "def transcribe_uploaded_audio(base64_audio, filename):\n",
        "    \"\"\"Process and transcribe uploaded audio file\"\"\"\n",
        "    try:\n",
        "        # Decode base64 audio\n",
        "        audio_data = base64.b64decode(base64_audio)\n",
        "\n",
        "        # Use pydub to handle various audio formats\n",
        "        audio_io = io.BytesIO(audio_data)\n",
        "\n",
        "        # Detect format from filename\n",
        "        file_extension = filename.split('.')[-1].lower()\n",
        "        format_map = {\n",
        "            'mp3': 'mp3',\n",
        "            'wav': 'wav',\n",
        "            'webm': 'webm',\n",
        "            'm4a': 'mp4',\n",
        "            'ogg': 'ogg',\n",
        "            'flac': 'flac'\n",
        "        }\n",
        "\n",
        "        audio_format = format_map.get(file_extension, 'mp3')\n",
        "\n",
        "        # Load audio with pydub\n",
        "        audio_segment = AudioSegment.from_file(audio_io, format=audio_format)\n",
        "\n",
        "        # Convert to mono and set sample rate to 16kHz\n",
        "        audio_segment = audio_segment.set_channels(1).set_frame_rate(16000)\n",
        "\n",
        "        # Convert to numpy array\n",
        "        audio_np = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)\n",
        "\n",
        "        # Normalize\n",
        "        audio_float = audio_np / 32768.0\n",
        "\n",
        "        # Get duration\n",
        "        duration = len(audio_float) / 16000\n",
        "\n",
        "        print(f\"üìÅ Processing file: {filename}\")\n",
        "        print(f\"‚è±Ô∏è Duration: {duration:.2f} seconds\")\n",
        "        print(f\"üîä Sample rate: 16kHz\")\n",
        "        print(f\"üìä Audio length: {len(audio_float)} samples\")\n",
        "\n",
        "        # Process audio in chunks if it's long (>30 seconds)\n",
        "        if duration > 30:\n",
        "            return transcribe_long_audio(audio_float, filename)\n",
        "        else:\n",
        "            return transcribe_short_audio(audio_float, filename)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Error processing {filename}: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "def transcribe_short_audio(audio_float, filename):\n",
        "    \"\"\"Transcribe short audio files (<=30 seconds)\"\"\"\n",
        "    try:\n",
        "        # Transcribe with your model\n",
        "        inputs = processor(audio_float, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcription = processor.batch_decode(predicted_ids)[0].strip()\n",
        "\n",
        "        if transcription and len(transcription) > 1:\n",
        "            result = f\"‚úÖ Transcription for {filename}:\\nüó£Ô∏è Malayalam: {transcription}\"\n",
        "            print(result)\n",
        "            return result\n",
        "        else:\n",
        "            result = f\"‚ö†Ô∏è No clear speech detected in {filename}\"\n",
        "            print(result)\n",
        "            return result\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Error transcribing {filename}: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "def transcribe_long_audio(audio_float, filename):\n",
        "    \"\"\"Transcribe long audio files (>30 seconds) in chunks\"\"\"\n",
        "    try:\n",
        "        chunk_length = 30 * 16000  # 30 seconds in samples\n",
        "        chunks = []\n",
        "        transcriptions = []\n",
        "\n",
        "        # Split audio into chunks\n",
        "        for i in range(0, len(audio_float), chunk_length):\n",
        "            chunk = audio_float[i:i + chunk_length]\n",
        "            if len(chunk) > 8000:  # At least 0.5 seconds\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        print(f\"üîÑ Processing {len(chunks)} chunks for {filename}...\")\n",
        "\n",
        "        # Process each chunk\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            try:\n",
        "                inputs = processor(chunk, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits = model(**inputs).logits\n",
        "\n",
        "                predicted_ids = torch.argmax(logits, dim=-1)\n",
        "                transcription = processor.batch_decode(predicted_ids)[0].strip()\n",
        "\n",
        "                if transcription and len(transcription) > 1:\n",
        "                    transcriptions.append(f\"[{i+1:02d}] {transcription}\")\n",
        "                    print(f\"üìù Chunk {i+1}/{len(chunks)}: {transcription}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error processing chunk {i+1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if transcriptions:\n",
        "            full_transcription = \"\\n\".join(transcriptions)\n",
        "            result = f\"‚úÖ Complete transcription for {filename}:\\n{full_transcription}\"\n",
        "            return result\n",
        "        else:\n",
        "            result = f\"‚ö†Ô∏è No clear speech detected in {filename}\"\n",
        "            return result\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Error processing long audio {filename}: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "# Register callbacks for JavaScript\n",
        "output.register_callback('transcribe_audio', transcribe_realtime_audio)\n",
        "output.register_callback('transcribe_file', transcribe_uploaded_audio)\n",
        "\n",
        "# Complete interface with both real-time and file upload\n",
        "complete_interface = HTML(\"\"\"\n",
        "<div style=\"max-width: 800px; margin: 0 auto; font-family: Arial, sans-serif;\">\n",
        "    <h2 style=\"text-align: center; color: #2E7D32;\">üé§ Malayalam Audio Transcription System</h2>\n",
        "\n",
        "    <!-- Real-time Recording Section -->\n",
        "    <div style=\"margin: 20px 0; padding: 20px; border: 2px solid #4CAF50; border-radius: 10px; background-color: #f9f9f9;\">\n",
        "        <h3 style=\"color: #2E7D32; margin-top: 0;\">üî¥ Real-time Recording</h3>\n",
        "        <button id=\"startBtn\" onclick=\"startRealTimeRecording()\" style=\"background-color: #4CAF50; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; margin: 5px;\">\n",
        "            Start Real-time Recording\n",
        "        </button>\n",
        "        <button id=\"stopBtn\" onclick=\"stopRealTimeRecording()\" disabled style=\"background-color: #f44336; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; margin: 5px;\">\n",
        "            Stop Recording\n",
        "        </button>\n",
        "        <div id=\"realtime-status\" style=\"font-size: 16px; font-weight: bold; margin: 10px 0;\">Ready to record Malayalam audio...</div>\n",
        "        <div id=\"realtime-transcription\" style=\"margin-top: 10px; padding: 10px; background-color: white; border: 1px solid #ddd; border-radius: 5px; min-height: 80px; max-height: 200px; overflow-y: auto;\"></div>\n",
        "    </div>\n",
        "\n",
        "    <!-- File Upload Section -->\n",
        "    <div style=\"margin: 20px 0; padding: 20px; border: 2px solid #2196F3; border-radius: 10px; background-color: #f3f8ff;\">\n",
        "        <h3 style=\"color: #1976D2; margin-top: 0;\">üìÅ Audio File Upload</h3>\n",
        "        <div style=\"margin: 15px 0;\">\n",
        "            <input type=\"file\" id=\"audioFileInput\" accept=\"audio/*\" style=\"margin: 10px 0; padding: 5px;\">\n",
        "            <button id=\"uploadBtn\" onclick=\"uploadAndTranscribe()\" style=\"background-color: #2196F3; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px;\">\n",
        "                Upload & Transcribe\n",
        "            </button>\n",
        "        </div>\n",
        "        <div style=\"font-size: 14px; color: #666;\">\n",
        "            Supported formats: MP3, WAV, M4A, OGG, FLAC, WebM (Max size: 100MB)\n",
        "        </div>\n",
        "        <div id=\"upload-status\" style=\"font-size: 16px; font-weight: bold; margin: 10px 0;\">Select an audio file to transcribe...</div>\n",
        "        <div id=\"upload-transcription\" style=\"margin-top: 10px; padding: 10px; background-color: white; border: 1px solid #ddd; border-radius: 5px; min-height: 80px; max-height: 300px; overflow-y: auto; white-space: pre-wrap;\"></div>\n",
        "    </div>\n",
        "\n",
        "    <!-- Instructions -->\n",
        "    <div style=\"margin: 20px 0; padding: 15px; background-color: #fff3e0; border-radius: 5px; border-left: 4px solid #ff9800;\">\n",
        "        <h4 style=\"color: #ef6c00; margin-top: 0;\">üìã Instructions:</h4>\n",
        "        <ul style=\"color: #bf360c;\">\n",
        "            <li><strong>Real-time:</strong> Click \"Start Recording\" and speak in Malayalam</li>\n",
        "            <li><strong>File Upload:</strong> Select an audio file and click \"Upload & Transcribe\"</li>\n",
        "            <li>Long files (>30 seconds) will be processed in chunks</li>\n",
        "            <li>Ensure clear audio quality for better transcription</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<script>\n",
        "// Real-time recording variables\n",
        "let mediaRecorder;\n",
        "let isRecording = false;\n",
        "let audioChunks = [];\n",
        "let recordingInterval;\n",
        "\n",
        "// Real-time recording functions\n",
        "async function startRealTimeRecording() {\n",
        "    try {\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({\n",
        "            audio: {\n",
        "                sampleRate: 16000,\n",
        "                channelCount: 1,\n",
        "                echoCancellation: true,\n",
        "                noiseSuppression: true\n",
        "            }\n",
        "        });\n",
        "\n",
        "        mediaRecorder = new MediaRecorder(stream, {\n",
        "            mimeType: 'audio/webm;codecs=opus'\n",
        "        });\n",
        "\n",
        "        isRecording = true;\n",
        "        document.getElementById('startBtn').disabled = true;\n",
        "        document.getElementById('stopBtn').disabled = false;\n",
        "        document.getElementById('realtime-status').innerHTML = 'üî¥ Recording... Speak in Malayalam!';\n",
        "        document.getElementById('realtime-transcription').innerHTML = '';\n",
        "\n",
        "        // Process audio in chunks every 3 seconds\n",
        "        recordingInterval = setInterval(() => {\n",
        "            if (isRecording && mediaRecorder.state === 'recording') {\n",
        "                mediaRecorder.stop();\n",
        "                setTimeout(() => {\n",
        "                    if (isRecording) {\n",
        "                        mediaRecorder.start();\n",
        "                    }\n",
        "                }, 100);\n",
        "            }\n",
        "        }, 3000);\n",
        "\n",
        "        mediaRecorder.ondataavailable = function(event) {\n",
        "            if (event.data.size > 0) {\n",
        "                audioChunks.push(event.data);\n",
        "                processRealtimeAudioChunk(event.data);\n",
        "            }\n",
        "        };\n",
        "\n",
        "        mediaRecorder.start();\n",
        "\n",
        "    } catch (err) {\n",
        "        console.error('Error accessing microphone:', err);\n",
        "        document.getElementById('realtime-status').innerHTML = '‚ùå Error: Cannot access microphone';\n",
        "    }\n",
        "}\n",
        "\n",
        "function stopRealTimeRecording() {\n",
        "    isRecording = false;\n",
        "\n",
        "    if (mediaRecorder && mediaRecorder.state !== 'inactive') {\n",
        "        mediaRecorder.stop();\n",
        "        mediaRecorder.stream.getTracks().forEach(track => track.stop());\n",
        "    }\n",
        "\n",
        "    if (recordingInterval) {\n",
        "        clearInterval(recordingInterval);\n",
        "    }\n",
        "\n",
        "    document.getElementById('startBtn').disabled = false;\n",
        "    document.getElementById('stopBtn').disabled = true;\n",
        "    document.getElementById('realtime-status').innerHTML = '‚èπÔ∏è Recording stopped';\n",
        "}\n",
        "\n",
        "async function processRealtimeAudioChunk(audioBlob) {\n",
        "    try {\n",
        "        const reader = new FileReader();\n",
        "        reader.onload = function() {\n",
        "            const base64Audio = reader.result.split(',')[1];\n",
        "            google.colab.kernel.invokeFunction('transcribe_audio', [base64Audio], {})\n",
        "                .then(result => {\n",
        "                    if (result.data && result.data.trim()) {\n",
        "                        const transcriptionDiv = document.getElementById('realtime-transcription');\n",
        "                        transcriptionDiv.innerHTML += result.data + '<br>';\n",
        "                        transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;\n",
        "                    }\n",
        "                });\n",
        "        };\n",
        "        reader.readAsDataURL(audioBlob);\n",
        "    } catch (error) {\n",
        "        console.error('Error processing real-time audio chunk:', error);\n",
        "    }\n",
        "}\n",
        "\n",
        "// File upload functions\n",
        "async function uploadAndTranscribe() {\n",
        "    const fileInput = document.getElementById('audioFileInput');\n",
        "    const file = fileInput.files[0];\n",
        "\n",
        "    if (!file) {\n",
        "        document.getElementById('upload-status').innerHTML = '‚ö†Ô∏è Please select an audio file first';\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Validate file size (100MB max)\n",
        "    if (file.size > 100 * 1024 * 1024) {\n",
        "        document.getElementById('upload-status').innerHTML = '‚ùå File too large. Maximum size is 100MB';\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Validate file type\n",
        "    const allowedTypes = ['audio/mpeg', 'audio/wav', 'audio/mp4', 'audio/ogg', 'audio/webm', 'audio/flac', 'audio/m4a'];\n",
        "    if (!allowedTypes.some(type => file.type.includes('audio'))) {\n",
        "        document.getElementById('upload-status').innerHTML = '‚ùå Please select a valid audio file';\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    document.getElementById('upload-status').innerHTML = 'üîÑ Processing audio file... Please wait';\n",
        "    document.getElementById('upload-transcription').innerHTML = 'Processing...';\n",
        "    document.getElementById('uploadBtn').disabled = true;\n",
        "\n",
        "    try {\n",
        "        const reader = new FileReader();\n",
        "        reader.onload = function() {\n",
        "            const base64Audio = reader.result.split(',')[1];\n",
        "            google.colab.kernel.invokeFunction('transcribe_file', [base64Audio, file.name], {})\n",
        "                .then(result => {\n",
        "                    document.getElementById('upload-status').innerHTML = '‚úÖ Transcription completed';\n",
        "                    document.getElementById('upload-transcription').innerHTML = result.data || 'No transcription result';\n",
        "                    document.getElementById('uploadBtn').disabled = false;\n",
        "                })\n",
        "                .catch(error => {\n",
        "                    console.error('Transcription error:', error);\n",
        "                    document.getElementById('upload-status').innerHTML = '‚ùå Error during transcription';\n",
        "                    document.getElementById('upload-transcription').innerHTML = 'Transcription failed. Please try again.';\n",
        "                    document.getElementById('uploadBtn').disabled = false;\n",
        "                });\n",
        "        };\n",
        "        reader.readAsDataURL(file);\n",
        "    } catch (error) {\n",
        "        console.error('File reading error:', error);\n",
        "        document.getElementById('upload-status').innerHTML = '‚ùå Error reading file';\n",
        "        document.getElementById('uploadBtn').disabled = false;\n",
        "    }\n",
        "}\n",
        "\n",
        "// File input change handler\n",
        "document.addEventListener('DOMContentLoaded', function() {\n",
        "    const fileInput = document.getElementById('audioFileInput');\n",
        "    if (fileInput) {\n",
        "        fileInput.addEventListener('change', function() {\n",
        "            const file = this.files[0];\n",
        "            if (file) {\n",
        "                const fileSize = (file.size / (1024 * 1024)).toFixed(2);\n",
        "                document.getElementById('upload-status').innerHTML =\n",
        "                    `üìÅ Selected: ${file.name} (${fileSize} MB) - Ready to transcribe`;\n",
        "            }\n",
        "        });\n",
        "    }\n",
        "});\n",
        "</script>\n",
        "\"\"\")\n",
        "\n",
        "# Display the complete interface\n",
        "print(\"üöÄ Complete Malayalam Speech Recognition System\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üî¥ Real-time: Start recording and speak in Malayalam\")\n",
        "print(\"üìÅ File Upload: Select and upload audio files for transcription\")\n",
        "print(\"üîÑ Long files will be processed in chunks automatically\")\n",
        "print(\"=\" * 60)\n",
        "display(complete_interface)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbnjyTzR4uZN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}